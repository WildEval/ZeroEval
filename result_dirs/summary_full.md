| Model                            |   GSM8K |   MMLU<br/>-Redux |   ZebraLogic<br/>-Easy |   CRUX |   MATH<br/>-L5 |
|:---------------------------------|--------:|------------------:|-----------------------:|-------:|---------------:|
| Llama-3.1-405B-Inst-fp8@together |   95.91 |             85.64 |                  87.14 |  72.12 |         nan    |
| Llama-3.1-405B-Inst@hyperbolic   |   95.98 |             84.41 |                 nan    |  71.50 |         nan    |
| gpt-4o-2024-08-06                |   96.21 |             88.26 |                  84.64 |  85.00 |          55.34 |
| gemini-1.5-pro-exp-0801          |   95.00 |             85.53 |                  72.50 |  73.00 |         nan    |
| claude-3-5-sonnet-20240620       |   95.60 |             86.00 |                  87.50 |  78.75 |          51.87 |
| gpt-4o-2024-05-13                |   95.38 |             88.01 |                  77.86 |  83.62 |          54.79 |
| deepseek-v2-coder-0614           |   93.78 |             79.63 |                  64.64 | nan    |         nan    |
| deepseek-v2-chat-0628            |   93.93 |             80.81 |                  68.57 |  68.50 |         nan    |
| gemini-1.5-pro-exp-0827          |  nan    |             86.14 |                  79.64 |  77.62 |          68.10 |
| Llama-3.1-405B-Inst@sambanova    |   95.91 |             86.21 |                  84.64 |  71.25 |          49.79 |
| chatgpt-4o-latest-24-09-07       |  nan    |             88.88 |                  81.43 |  84.25 |          53.12 |
| Mistral-Large-2                  |   95.53 |             82.97 |                  80.36 |  72.88 |          48.54 |
| deepseek-v2-coder-0724           |   91.51 |             80.24 |                  61.79 |  67.75 |         nan    |
| gpt-4o-mini-2024-07-18           |   94.24 |             81.50 |                  62.50 |  73.50 |          52.15 |
| claude-3-sonnet-20240229         |   91.51 |             74.87 |                  58.93 |  64.75 |         nan    |
| claude-3-opus-20240229           |   95.60 |             82.54 |                  78.21 |  68.62 |          36.89 |
| gpt-4-turbo-2024-04-09           |  nan    |             85.31 |                  80.71 |  76.75 |          46.46 |
| Meta-Llama-3.1-70B-Instruct      |   94.16 |             82.97 |                  73.57 |  62.62 |          43.13 |
| deepseek-v2.5-0908               |   92.49 |             80.35 |                  68.21 |  68.12 |          44.66 |
| yi-large-preview                 |   82.64 |             82.15 |                  58.93 |  58.63 |         nan    |
| yi-large                         |   80.06 |             81.17 |                  58.21 |  58.38 |         nan    |
| gemini-1.5-pro                   |   93.40 |             82.76 |                  55.71 |  66.25 |          39.81 |
| Qwen2-72B-Instruct               |   92.65 |             81.61 |                  63.93 |  57.38 |          38.28 |
| gemini-1.5-flash                 |   91.36 |             77.36 |                  59.29 |  61.88 |          34.81 |
| command-r-plus                   |   80.14 |             68.61 |                  44.64 | nan    |         nan    |
| gpt-4-0314                       |  nan    |             81.64 |                  77.14 |  72.38 |          26.07 |
| Mistral-Nemo-Instruct-2407       |   82.79 |             66.88 |                  38.93 | nan    |         nan    |
| Meta-Llama-3-70B-Instruct        |   93.03 |             78.01 |                  52.86 |  57.12 |          25.10 |
| gemma-2-27b-it                   |   90.22 |             75.67 |                  50.71 |  55.88 |          26.63 |
| Athene-70B                       |   86.66 |             76.64 |                  52.50 |  49.75 |          20.67 |
| Llama-3-Instruct-8B-SimPO-v0.2   |   57.54 |             55.22 |                 nan    | nan    |         nan    |
| claude-3-haiku-20240307          |   88.78 |             72.32 |                  47.86 |  53.62 |          15.12 |
| reka-core-20240501               |   87.41 |             76.42 |                  43.21 |  45.00 |          21.91 |
| gemma-2-9b-it                    |   87.41 |             72.82 |                  41.79 |  44.88 |          19.42 |
| Mixtral-8x7B-Instruct-v0.1       |   70.13 |             63.17 |                  28.93 |  43.50 |         nan    |
| Meta-Llama-3.1-8B-Instruct       |   84.00 |             67.24 |                  43.57 |  38.75 |          22.19 |
| Yi-1.5-34B-Chat                  |   84.08 |             72.79 |                  37.50 |  42.88 |          18.17 |
| reka-flash-20240226              |   74.68 |             64.72 |                  30.71 |  33.25 |         nan    |
| gpt-3.5-turbo-0125               |   80.36 |             68.36 |                  33.57 |  53.25 |          13.73 |
| Phi-3-mini-4k-instruct           |   75.51 |             70.34 |                  38.21 |  43.50 |          16.23 |
| command-r                        |   52.99 |             61.12 |                  32.14 | nan    |         nan    |
| Qwen2-7B-Instruct                |   80.06 |             66.92 |                  29.29 |  36.75 |          23.86 |
| Phi-3.5-mini-instruct            |   82.03 |             67.67 |                  21.79 |  40.88 |          18.72 |
| Meta-Llama-3-8B-Instruct         |   78.47 |             61.66 |                  40.71 |  36.62 |           7.91 |
| Yi-1.5-9B-Chat                   |   76.42 |             65.05 |                   8.21 |  43.75 |          19.97 |
| Qwen2-1.5B-Instruct              |   43.37 |             41.11 |                 nan    | nan    |         nan    |
| Rex-v0.1-1.5B                    |  nan    |             39.09 |                 nan    | nan    |         nan    |
| mathstral-7B-v0.1                |  nan    |            nan    |                  30.00 | nan    |         nan    |
| gemma-2-2b-it                    |   51.63 |             51.94 |                  14.29 |  20.75 |           4.30 |